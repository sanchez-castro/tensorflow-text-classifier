{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow classifier\n",
    "\n",
    "[Github Repo](https://github.com/GoogleCloudPlatform/ai-platform-text-classifier-shap/blob/master/stackoverflow-classifier.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloudml-demo-lcm/SO_ml_tags_avocado_188k_v2.csv...\n",
      "- [1 files][276.7 MiB/276.7 MiB]                                                \n",
      "Operation completed over 1 objects/276.7 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp 'gs://cloudml-demo-lcm/SO_ml_tags_avocado_188k_v2.csv' ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO_ml_tags_avocado_188k_v2.csv\ttext-classifier.ipynb  tutorials\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('SO_ml_tags_avocado_188k_v2.csv', names=['tags', 'original_tags', 'text'], header=0)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['original_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of any order inherited from the table\n",
    "data = shuffle(data, random_state = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182914</th>\n",
       "      <td>tensorflow,keras</td>\n",
       "      <td>avocado image captioning model not compiling b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48361</th>\n",
       "      <td>pandas</td>\n",
       "      <td>return excel file from avocado with flask in f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181447</th>\n",
       "      <td>tensorflow,keras</td>\n",
       "      <td>validating with generator (avocado) i'm trying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66307</th>\n",
       "      <td>pandas</td>\n",
       "      <td>avocado multiindex dataframe selecting data gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11283</th>\n",
       "      <td>pandas</td>\n",
       "      <td>get rightmost non-zero value position for each...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tags                                               text\n",
       "182914  tensorflow,keras  avocado image captioning model not compiling b...\n",
       "48361             pandas  return excel file from avocado with flask in f...\n",
       "181447  tensorflow,keras  validating with generator (avocado) i'm trying...\n",
       "66307             pandas  avocado multiindex dataframe selecting data gi...\n",
       "11283             pandas  get rightmost non-zero value position for each..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'avocado image captioning model not compiling because of concatenate layer when mask_zero=true in a previous layer i am new to avocado and i am trying to implement a model for an image captioning project.   i am trying to reproduce the model from image captioning pre-inject architecture (the picture is taken from this paper: where to put the image in an image captioning generator) (but with a minor difference: generating a word at each time step instead of only generating a single word at the end), in which the inputs for the lstm at the first time step are the embedded cnn features. the lstm should support variable input length and in order to do this i padded all the sequences with zeros so that all of them have maxlen time steps.  the code for the model i have right now is the following:    def get_model(model_name, batch_size, maxlen, voc_size, embed_size,          cnn_feats_size, dropout_rate):      # create input layer for the cnn features     cnn_feats_input = input(shape=(cnn_feats_size,))      # normalize cnn features      normalized_cnn_feats = batchnormalization(axis=-1)(cnn_feats_input)      # embed cnn features to have same dimension with word embeddings     embedded_cnn_feats = dense(embed_size)(normalized_cnn_feats)      # add time dimension so that this layer output shape is (none, 1, embed_size)     final_cnn_feats = repeatvector(1)(embedded_cnn_feats)      # create input layer for the captions (each caption has max maxlen words)     caption_input = input(shape=(maxlen,))      # embed the captions     embedded_caption = embedding(input_dim=voc_size,                                  output_dim=embed_size,                                  input_length=maxlen)(caption_input)      # concatenate cnn features and the captions.     # ouput shape should be (none, maxlen + 1, embed_size)     img_caption_concat = concatenate([final_cnn_feats, embedded_caption], axis=1)      # now feed the concatenation into a lstm layer (many-to-many)     lstm_layer = lstm(units=embed_size,                       input_shape=(maxlen + 1, embed_size),   # one additional time step for the image features                       return_sequences=true,                       dropout=dropout_rate)(img_caption_concat)      # create a fully connected layer to make the predictions     pred_layer = timedistributed(dense(units=voc_size))(lstm_layer)      # build the model with cnn features and captions as input and      # predictions output     model = model(inputs=[cnn_feats_input, caption_input],                    outputs=pred_layer)      optimizer = adam(lr=0.0001,                       beta_1=0.9,                       beta_2=0.999,                       epsilon=1e-8)      model.compile(loss=\\'categorical_crossentropy\\',optimizer=optimizer)     model.summary()      return model   the model (as it is above) compiles without any errors (see: model summary) and i managed to train it using my data. however, it doesn\\'t take into account the fact that my sequences are zero-padded and the results won\\'t be accurate because of this. when i try to change the embedding layer in order to support masking (also making sure that i use voc_size + 1 instead of voc_size, as it\\'s mentioned in the documentation) like this:  embedded_caption = embedding(input_dim=voc_size + 1,                              output_dim=embed_size,                              input_length=maxlen, mask_zero=true)(caption_input)   i get the following error:  traceback (most recent call last):   file \"/export/home/.../py3_env/lib/python3.5/site-packages/avocado/python/framework/ops.py\", line 1567, in _create_c_op     c_op = c_api.avocado_finishoperation(op_desc) avocado.python.framework.errors_impl.invalidargumenterror: dimension 0 in both shapes must be equal, but are 200 and 1. shapes are [200] and [1]. for \\'concatenate_1/concat_1\\' (op: \\'concatv2\\') with input shapes: [?,1,200], [?,25,1], [] and with computed input tensors: input[2] = &lt;1&gt;   i don\\'t know why it says the shape of the second array is [?, 25, 1], as i am printing its shape before the concatenation and it\\'s [?, 25, 200] (as it should be).  i don\\'t understand why there\\'d be an issue with a model that compiles and works fine without that parameter, but i assume there\\'s something i am missing.  i have also been thinking about using a masking layer instead of mask_zero=true, but it should be before the embedding and the documentation says that the embedding layer should be the first layer in a model (after the input).   is there anything i could change in order to fix this or is there a workaround to this ?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tensorflow', 'keras']\n"
     ]
    }
   ],
   "source": [
    "# Encode top tags to multi-hot\n",
    "tags_split = [tags.split(',') for tags in data['tags'].values]\n",
    "print(tags_split[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keras' 'matplotlib' 'pandas' 'scikitlearn' 'tensorflow']\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "tag_encoder = MultiLabelBinarizer()\n",
    "tags_encoded = tag_encoder.fit_transform(tags_split)\n",
    "num_tags = len(tags_encoded[0])\n",
    "print(tag_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label vector of the first row\n",
    "tags_encoded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 150559\n",
      "Test size: 37640\n"
     ]
    }
   ],
   "source": [
    "# Split our data into train and test sets from the label tags\n",
    "train_size = int(len(data) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = tags_encoded[:train_size]\n",
    "test_tags = tags_encoded[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Creating a class to import in the future**\n",
    "[Keras preprocessing text method](https://keras.io/preprocessing/text/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "# Pre-processing data: create our tokenizer class\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "class TextPreprocessor(object):\n",
    "  def __init__(self, vocab_size):\n",
    "    self._vocab_size = vocab_size\n",
    "    self._tokenizer = None\n",
    "  \n",
    "  def create_tokenizer(self, text_list):\n",
    "    \"\"\"\n",
    "    This class allows to vectorize a text corpus, by turning each text into either a sequence of \n",
    "    integers (each integer being the index of a token in a dictionary) or into a vector where the \n",
    "    coefficient for each token could be binary, based on word count, based on tf-idf.\n",
    "    \"\"\"\n",
    "    tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
    "    tokenizer.fit_on_texts(text_list)\n",
    "    self._tokenizer = tokenizer\n",
    "\n",
    "  def transform_text(self, text_list):\n",
    "    text_matrix = self._tokenizer.texts_to_matrix(text_list)\n",
    "    return text_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab from training corpus\n",
    "from preprocess import TextPreprocessor\n",
    "\n",
    "VOCAB_SIZE = 400 # This is a hyperparameter, try out different values for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the train/test split\n",
    "train_qs = data['text'].values[:train_size]\n",
    "test_qs = data['text'].values[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the class\n",
    "processor = TextPreprocessor(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the matrix with the words size and the corpus of train qs\n",
    "processor.create_tokenizer(train_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the bag of words\n",
    "body_train = processor.transform_text(train_qs)\n",
    "body_test = processor.transform_text(test_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#print the size of the matrix & the first vector of the corpus in train\n",
    "print(len(body_train[0]))\n",
    "print(body_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processor state of the tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('./processor_state.pkl', 'wb') as f:\n",
    "  pickle.dump(processor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the neural net \n",
    "\n",
    "def create_model(vocab_size, num_tags):\n",
    "    \n",
    "    #Model groups layers into an object with training and inference features.\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    #Input shape = sizeof our matrix vector bag of words\n",
    "    model.add(tf.keras.layers.Dense(50, input_shape=(VOCAB_SIZE,), activation='relu'))\n",
    "    #A hidden layer to 25 nodes\n",
    "    model.add(tf.keras.layers.Dense(25, activation='relu'))\n",
    "    #Output layer to the number of tags that we want to predict\n",
    "    model.add(tf.keras.layers.Dense(num_tags, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                20050     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 21,455\n",
      "Trainable params: 21,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(VOCAB_SIZE, num_tags)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135503 samples, validate on 15056 samples\n",
      "Epoch 1/3\n",
      "135503/135503 [==============================] - 2s 18us/sample - loss: 0.0968 - acc: 0.9620 - val_loss: 0.0981 - val_acc: 0.9612\n",
      "Epoch 2/3\n",
      "135503/135503 [==============================] - 3s 19us/sample - loss: 0.0935 - acc: 0.9630 - val_loss: 0.0966 - val_acc: 0.9620\n",
      "Epoch 3/3\n",
      "135503/135503 [==============================] - 3s 19us/sample - loss: 0.0908 - acc: 0.9642 - val_loss: 0.0966 - val_acc: 0.9613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb32c7fe10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "#_train = input bag of words's array\n",
    "#_tags \n",
    "#epochs =  times where the model will iterate through the entire \n",
    "#batch size = how many elements the model will look at a time to update weights\n",
    "#validation split = validation size \n",
    "\n",
    "model.fit(body_train, train_tags, epochs=3, batch_size=128, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37640/37640 [==============================] - 0s 9us/sample - loss: 0.1002 - acc: 0.9604\n",
      "Eval loss/accuracy:[0.10022197399648672, 0.96041423]\n",
      "37640/37640 [==============================] - 0s 9us/sample - loss: 0.1002 - acc: 0.9604\n",
      "Eval loss/accuracy:[0.10022197399648672, 0.96041423]\n"
     ]
    }
   ],
   "source": [
    "print('Eval loss/accuracy:{}'.format(\n",
    "  model.evaluate(body_test, test_tags, batch_size=128)))\n",
    "\n",
    "print('Eval loss/accuracy:{}'.format(model.evaluate(body_test, test_tags, batch_size=128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to a file\n",
    "model.save('keras_saved_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our model (locally)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
